# -*- coding: utf-8 -*-
"""rossmann.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vBD-mAdgZR3U29-6dbZQhZN9XVqICIcg

#Machine Learning Course Assignment#

*Prof: Ladle Patel*

*Student: Pietro Barizza*

*Case study 1: Rossmann Store Sales*

*Objective*

You are provided with historical sales data of 1,115 Rossmann stores. Your task is to build predictive models to forecast sales based on a few relevant features using:
* Linear Regression
* Decision Tree Regressor
"""

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Q1. Load the train.csv dataset and display the first 5 rows."""

# load dataset from github
df = pd.read_csv('https://raw.githubusercontent.com/pbarizza/ML-course-2025/refs/heads/main/assignment/RossmannSalesData.csv')

# explore the dataset
df.head()

"""Q2. What is the shape of the dataset?"""

df.shape

df.columns

df.dtypes

df.info()

df.describe()

"""The dataset shape is 1149 rows by 9 columns:
  'Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday'

All columns are integers, apart from Data that is a string.

Q3. Which columns contain missing values? How will you handle them?
"""

print(df.isnull().sum())

"""There are no missing values
If there were there are several way to treat missing values:
- drop, if relevanet information are missing e.g. Sales
- replace with a median / mean / mode depending on the nature of feature
- estimate their distance using KNN
- replace with a default value if not particularly relevant

Q4. Filter out rows where the store was closed (Open == 0) or sales are 0. Why is this
step important?
"""

df_open = df[(df['Open'] != 0) | (df['Sales'] != 0)]
df_open['Store'].count()

"""There are 1147 open shops and two closed.
This is important otherwise they sales forecast will be affected, even if in this case the numbers are almost negligible.

Q5. From the available features, select the following for modeling:
* Store
* DayOfWeek
* Promo
* SchoolHoliday

Why are these features relevant?
"""

# extract features and target
# Sales is our target
features = ['Store', 'DayOfWeek', 'Promo', 'SchoolHoliday']
X = df_open[features]
y = df_open['Sales']
X.sample(5)

"""'Store', 'DayOfWeek', 'Promo', 'SchoolHoliday' are the independent valirables. They are the key parameters to model the regression. Sales indeed depends on the store, the day of the week, if a promotion is active and if it is a school holiday *day*.

Q6. Encode any categorical features (e.g., DayOfWeek) appropriately.
"""

# replace categorical day of the week into integers
# assumption is that days are expressed 'Ddd'.
days = {'Sun': 0, 'Mon': 1, 'Tue': 2, 'Wed': 3, 'Thu': 4, 'Fri': 5, 'Sat': 6}
X['DayOfWeek'] = X['DayOfWeek'].replace(days)
X['DayOfWeek'].value_counts()

"""DayOfWeek is already encoded as integer. However, the cell 40 provides the code to encore strings into integer e.g. 'Sun' into 0

Q7. Split the dataset into:
* Feature matrix X (selected features)
* Target variable y (Sales column)
"""

# see cell 41
X.sample(5)

y.sample(5)

"""Q8. Split the data into training and testing sets (e.g., 80/20 split)."""

# split data in 80/20
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape

X_test.shape

y_train.shape

y_test.shape

"""Q9. Scale the features using StandardScaler for Linear Regression. Why is scaling
important?
"""

# scale the data using the StandardScaler
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
# reuse the same mean / stdev, so just transform()
X_test_scaled = scaler.transform(X_test)
# reshape, since StandardScaler  expect a 2x2 array not a vector
y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))
# reuse the same mean / stdev, so just transform()
y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))

X_train_scaled[:5]

X_test_scaled[:5]

y_train_scaled[:5]

y_test_scaled[:5]

"""Scaling is important since the different features are measure in different unit of measurement, but sklearn (and in general any algorithm) is not aware of the unit. additionally features should be comparable, otherwise the gradients vanish or explode

Q10. Train a Linear Regression model using the scaled training data.
"""

# import the Linear Regression class
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
# train data on the 80% split
# note scaled data is used
lr.fit(X_train_scaled, y_train_scaled)

"""Q11. Train a Decision Tree Regressor with a max depth of 6 (no scaling needed here)."""

# import Decision Tree class
from sklearn.tree import DecisionTreeRegressor

# max_depth = 6
dt = DecisionTreeRegressor(max_depth=6, random_state=42)
# train on not scaled data
dt.fit(X_train, y_train)

"""Q12. Predict on the test set using both models"""

# predict on the 20% split using Linear Regression model
y_pred_scaled_lr = lr.predict(X_test_scaled)
# inverse transform
y_pred_lr = scaler.inverse_transform(y_pred_scaled_lr)
y_pred_lr[:5]

# predict on the 20% split using Decision Tree model
y_pred_dt = dt.predict(X_test)
y_pred_dt[:5]

# compare prediction with the actual values
comparison = X_test.copy()
comparison['Actual'] = y_test
comparison['LR'] = y_pred_lr
comparison['DT'] = y_pred_dt

# add the % of offset from the actual value
comparison['LR%'] = (((comparison['LR'] - comparison['Actual']) / comparison['Actual']) * 100).round(2)
comparison['DT%'] = (((comparison['DT'] - comparison['Actual']) / comparison['Actual']) * 100).round(2)

comparison.sample(5)

"""Q13. Calculate the Mean Squared Error (MSE) and RÂ² Score for both models.
Which model performs better, and why?
"""

# import metrics
from sklearn.metrics import mean_squared_error, r2_score

# MSE for Linear Regression
mse_lr = mean_squared_error(y_test, y_pred_lr)
# R-squared for Linear Regression
r2_lr = r2_score(y_test, y_pred_lr)

# MSE for Decision Tree
mse_dt = mean_squared_error(y_test, y_pred_dt)
# R-squared for Decision Tree
r2_dt = r2_score(y_test, y_pred_dt)


print(f'{"MSE/LR":<20} : {mse_lr}')
print(f'{"R-squared/LR":<20} : {r2_lr}')
print(f'{"MSE/DT":<20} : {mse_dt}')
print(f'{"R-squared/DT":<20} : {r2_dt}')



"""R^2 negative indicate the model is performing badly.
In general R2 is telling us the model don't fit well, since the value is very close to 0.

MSE very high means a very large error in the predictions

. Plot Actual vs Predicted Sales for both models. What do these plots tell you?
"""

y_pred_lr_flat = y_pred_lr.flatten()
y_pred_dt_flat = y_pred_dt.flatten()

# Linear Regression
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred_lr_flat)
plt.xlabel('Actual Sales')
plt.ylabel('LR Predicted Sales')
plt.title('Actual vs. Predicted Sales (Linear Regression)')
plt.show()

# Decision Tree Regressor
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred_dt_flat)
plt.xlabel('Actual Sales')
plt.ylabel('DT Predicted Sales')
plt.title('Actual vs. Predicted Sales (Decision Tree)')
plt.show()

"""A good prediction should show the points aligned on the diagolal y_test = y_pred_*. These diagrams clearly show that the predictions are pretty bad, scattered all over the plane.

Q15. Plot the residuals (actual - predicted) for Linear Regression. Do they appear
normally distributed?
"""

# residuals for Linear Regression
residuals_lr = y_test - y_pred_lr_flat

# histogram of residuals
plt.figure(figsize=(10, 6))
sns.histplot(residuals_lr, kde=True)
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Distribution of Residuals (Linear Regression)')
plt.show()

# Q-Q plot of residuals to check for normality
import statsmodels.api as sm
sm.qqplot(residuals_lr, line='s')
plt.title('Q-Q Plot of Residuals (Linear Regression)')
plt.show()

"""The residuals are not normally distributed. data are positively skewed.

Q16. Which model generalizes better? Which is more interpretable?
"""

# analysis
print('\n\nAnalysis:')
if r2_lr > r2_dt:
    print('Linear Regression performs better based on R-squared.')
elif r2_dt > r2_lr:
    print('Decision Tree performs better based on R-squared.')
else:
    print('Both models have the same R-squared score.')

if mse_lr < mse_dt:
    print('Linear Regression performs better based on MSE.')
elif mse_dt < mse_lr:
    print('Decision Tree  performs better based on MSE.')
else:
    print('Both models have the same MSE.')

"""Linear regression generalizes better on little features like in the given dataset. Decison Tree tends to overfit on little features.

Linear regression is more interpretable as model since it associated a coefficent for each feature, while the Decision tree provide a global structure which increase in complexity with the number of nodes

Q17. What business insights can you draw from this analysis?

There are no business insights since both models perform very poorly, as shown by the very low R-squared and the very high MSE.
  The same indicate that the driving coefficient are missing in the equations. This is probably due to the choice of the features.

Q18. What limitations exist in this modeling approach?

1. The choice of the features is limited and posiible not capturing the business drivers

2. data is limited e.g. 2 days (4 and 5)

3. the residuals are not normally distributed. This violates the pre-requisites of the Linear Regression (OLS). Other models should be used isntead e.g. WLS or generalized models.

4. limited depth of the decision tree

Q19. How could you improve the model further? (e.g., more features, time series
models, etc.)

1. adding all features to the model

2. analyze the outliers and treat them, like capping them to get them normalized

3. use alternative Linear regression models e.g. WLS or GLS.

4. increase the depth of the decision tree or better opt for random forests regressor

5. use time series. Sales have typical temporal pattern. Time series algorithms like ARIMA or SARIMA can probably provide better results.
"""

